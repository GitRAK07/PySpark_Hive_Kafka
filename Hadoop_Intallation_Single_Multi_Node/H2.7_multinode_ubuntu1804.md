For the easy access to the machines have a host name for the each machine in the etc hosts file.
    /etc/hosts

    <ip> <name>
    10.20.3.1 master
    10.20.3.2 node1

Add a hadoop user in both machines.
    sudo adduser hadoop

Login to the master node as hadoop user and generate ssh key pairs.
    ssh-keygen -t rsa

Copy the public key to all data nodes and also to master node if you want use that as a datanode.
ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@master
ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@node1

if you get the error
ssh: connect t host slave-1 port 22: Connection refused

I'd generally take "connection refused" to mean that the target server isn't listening on port 22.

What's the output of 
netstat -an|grep 22 
when run on slave-1

it should show 
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN

if not then do
sudo apt-get install openssh-server openssh-client
Sometime the error is simple, ssh is not installed. the openssh-client should be installed on your local machine, and openssh-server should be installed on the remote computer. However, you can install both openssh-client and openssh-server on both machines


From Apache hadoop website Download Hadoop banaries not source in ubuntu user

    wget http://mirror.olnevhost.net/pub/apache/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz

NOTE: There is a possibility of below-mentioned error in this setup and installation process.

"hadoop is not in the sudoers file. This incident will be reported." 
This error can be resolved by Login as a root user 
su ubuntu
then write
sudo adduser hadoop sudo


    sudo wget http://mirror.olnevhost.net/pub/apache/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz
    sudo tar -xvf hadoop-2.7.7.tar.gz
    sudo mv hadoop-2.7.7.tar.gz hadoop

Login to hadoop user
Add hadoop path to ~/.bashrc  or ~/.profile file.

    PATH=/home/hadoop/hadoop/bin:/home/hadoop/hadoop/sbin:$PATH

Setting up the master node

We can modify the configuration files on master node and replicate them on other machines.

Setup JAVA_HOME environment varibale in hadoop/etc/hadoop/hadoop-env.sh. 

    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/



Edit the hadoop/etc/hadoop/core-site.xml. It should contain following xml code.

    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master</value> 
        <description>NameNode URI</description>
      </property>
    </configuration>

Change the hadoop/etc/hadoop/hdfs-site.xml

    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/home/hadoop_cluster_data/temp</value>
        </property>
        <property>
            <name>dfs.name.dir</name>
            <value>/home/hadoop_cluster_data/name</value>
        </property>
            <property>
            <name>dfs.data.dir</name>
            <value>/home/hadoop_cluster_data/data</value>
        </property>
    </configuration>

Add this to hadoop/etc/hadoop/mapred-site-template.xml

    <configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
    </configuration>

Add this to hadoop/etc/hadoop/yarn-site.xml

    <configuration>
        <property>
                <name>yarn.acl.enable</name>
                <value>0</value>
        </property>
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>node-master</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
    </configuration>

Edit hadoop/etc/hadoop/slaves

    master
    node1

Adding master also as slave because we are that also as data node.

Now, the configuration for hadoop is done.

Letâ€™s copy whole configuration to other machine.

    cd ~/hadoop
    scp -r $(pwd) hadoop@node1:~/hadoop

write in ubuntu user
hadoop version

if it shows the version then path is set properly, if not do
nano ~/.bashrc
export HADOOP_HOME="/home/ubuntu/Downloads/hadoop"
export PATH="$PATH:$HADOOP_HOME/bin"
source ~/.bashrc

do the same in slave node

Running HDFS

Run following to the format name node.

    hdfs namenode -format

Our hadoop configuration is ready to run. Start the dfs by running following command.

    start-dfs.sh
